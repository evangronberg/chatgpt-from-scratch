# Module 7 Short Answers

Read a paper that was not discussed in class lecture: the Longformer paper. This paper is one of many that have attempted to make a more efficient attention mechanism.

Read sections 1 through 4 of the paper:

https://arxiv.org/pdf/2004.05150

## Question 1

Why does the memory complexity of a transformer expand quadratically when the input sequence only expands linearly? How does this limit our ability to build larger and larger models?

**Answer**

TBA

## Question 2

How does the Longformer try to improve on this complexity?

**Answer**

TBA
